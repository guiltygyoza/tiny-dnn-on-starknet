{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f641863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "489db1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 32)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dbd1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.load_state_dict(torch.load('mnist_dnn.pt'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2275fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> printing the model\n",
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"> printing the model\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc8060c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> showing how many trainable parameters there are in the model\n",
      "25450\n"
     ]
    }
   ],
   "source": [
    "print(\"> showing how many trainable parameters there are in the model\")\n",
    "print( sum(p.numel() for p in model.parameters() if p.requires_grad) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93976d9",
   "metadata": {},
   "source": [
    "#### Model summary\n",
    "```\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x) <== skip for inference\n",
    "        x = self.fc2(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d14ae2c",
   "metadata": {},
   "source": [
    "## Manually dividing the model into contracts\n",
    "- **Future: build graph parser for pytorch models and auto-generate deployable cairo contracts + deployment script for address linkage, taking StarkNet constraints + parallelization scheme intended by the user**\n",
    "- FC1: X(1x784) * M1 (784x32) = V1 (1x32)\n",
    "    - dividing fc1 into 32 sop (sum-of-product; each being 784 multiply-add + bias), each producing one element in V1\n",
    "    - further dividing each sop into 8 sub-sop, first 7 having 100 terms each, while the 8th has 84 terms.\n",
    "        - **if not dividing into sub-sop's => would trigger exception on Lark's recursion depth limit; Lark is the parser used by the Cairo compiler by StarkWare**\n",
    "    - one contract per sub-sop\n",
    "    - **mnist_v1_sop<0-31>.cairo** one contract per sop, grouping 8 sub-sop's\n",
    "    - **mnist_v1.cairo**: one contract groups 32 sop's to V1\n",
    "    - => 1 + 32 + 32\\*8 = **289 contracts**\n",
    "- RELU: V1 (1x32) ==relu==> H1 (1x32)\n",
    "    - one contract to produce H1\n",
    "    - => **1 contract**\n",
    "- FC2: H1 (1x32) * M2 (32x10) = Z (1x10)\n",
    "    - dividing fc2 into 10 sop, each producing one element in Z\n",
    "    - one contract per sop (32 multiply-add + bias)\n",
    "    - => **10 contracts**\n",
    "- Total: **300 contracts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b1fa1",
   "metadata": {},
   "source": [
    "### Contract: V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b816c07",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Printing out cairo code for top-level contracts\n"
     ]
    }
   ],
   "source": [
    "print(\"> Printing out cairo code for top-level contracts\")\n",
    "\n",
    "# for i in range(32):\n",
    "#     print(f'    let (addr_{i}) = stored_addresses.read({i})')\n",
    "#     print(f'    local pedersen_ptr : HashBuiltin* = pedersen_ptr')\n",
    "#     print(f'    let (v1_{i}) = IContractV1SOP{i}.compute(addr_{i}, 784, x)')\n",
    "#     print(f'    assert [v1+{i}] = v1_{i}')\n",
    "#     print()\n",
    "\n",
    "# for i in range(32):\n",
    "#     print(f'@contract_interface')\n",
    "#     print(f'namespace IContractV1SOP{i}:')\n",
    "#     print(f'    func compute(x_len : felt, x : felt*) -> (res : felt):')\n",
    "#     print(f'    end')\n",
    "#     print(f'end')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06fbc79",
   "metadata": {},
   "source": [
    "### Contract: V1_SOP<0-31>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dac44960",
   "metadata": {},
   "outputs": [],
   "source": [
    "part1 = [\n",
    "'%lang starknet',\n",
    "'%builtins pedersen range_check',\n",
    "'',\n",
    "'from starkware.cairo.common.cairo_builtins import HashBuiltin',\n",
    "'from starkware.cairo.common.math import signed_div_rem',\n",
    "'',\n",
    "'@storage_var',\n",
    "'func stored_addresses (idx : felt) -> (addr : felt):',\n",
    "'end',\n",
    "'',\n",
    "'@external',\n",
    "'func admin_store_addresses {',\n",
    "'        syscall_ptr : felt*, pedersen_ptr : HashBuiltin*, range_check_ptr',\n",
    "'    } (idx : felt, addr : felt) -> ():',\n",
    "'    stored_addresses.write(idx, addr)',\n",
    "'    return ()',\n",
    "'end',\n",
    "'',\n",
    "'################################################',\n",
    "''\n",
    "]\n",
    "\n",
    "############\n",
    "\n",
    "def part2 (sop_idx):\n",
    "    ret = []\n",
    "    for i in range(8):\n",
    "        ret += [\n",
    "            '@contract_interface',\n",
    "            f'namespace IContractV1SOP{sop_idx}SUB{i}:',\n",
    "            '    func compute(x_len : felt, x : felt*) -> (res : felt):',\n",
    "            '    end',\n",
    "            'end',\n",
    "            ''\n",
    "        ]\n",
    "    ret.append('################################################')\n",
    "    ret.append('')\n",
    "    return ret\n",
    "\n",
    "############\n",
    "\n",
    "def part3 (sop_idx, quantize_depth=8):\n",
    "    ret = []\n",
    "    ret += [\n",
    "        '@view',\n",
    "        'func compute {',\n",
    "        '        syscall_ptr : felt*, pedersen_ptr : HashBuiltin*, range_check_ptr',\n",
    "        '    }(',\n",
    "        '        x_len : felt,',\n",
    "        '        x : felt*',\n",
    "        '    ) -> (',\n",
    "        '        res : felt',\n",
    "        '    ):',\n",
    "        '    alloc_locals',\n",
    "        ''\n",
    "    ]\n",
    "    \n",
    "    for i in range(8):\n",
    "        x_len = 100 if i!=7 else 84\n",
    "        ret.append(f'    let (addr_{i}) = stored_addresses.read({i})')\n",
    "        ret.append(f'    local pedersen_ptr : HashBuiltin* = pedersen_ptr')\n",
    "        ret.append(f'    let (local sub{i}) = IContractV1SOP{sop_idx}SUB{i}.compute(addr_{i}, {x_len}, x+100*{i})')\n",
    "        ret.append('')\n",
    "    ret.append('    let res_ = sub0 + sub1 + sub2 + sub3 + sub4 + sub5 + sub6 + sub7')\n",
    "    ret.append(f'    let (res, _) = signed_div_rem(res_, {10**quantize_depth}, 2 ** 64)')\n",
    "    ret.append('    return (res)')\n",
    "    ret.append('end')\n",
    "    ret.append('')\n",
    "\n",
    "    return ret\n",
    "\n",
    "def gen_v1_sop_contract(sop_idx, quantize_depth):\n",
    "    ret = []\n",
    "    ret += part1\n",
    "    ret += part2(sop_idx)\n",
    "    ret += part3(sop_idx, quantize_depth)\n",
    "    \n",
    "    with open(f'gen_contract/mnist_v1_sop{sop_idx}.cairo', 'a') as f:\n",
    "        for line in ret:\n",
    "            f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aec48374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Generating 32 sop contracts for V1\n"
     ]
    }
   ],
   "source": [
    "print(\"> Generating 32 sop contracts for V1\")\n",
    "# for sop_idx in range(32):\n",
    "#     gen_v1_sop_contract(sop_idx, quantize_depth=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd36c3",
   "metadata": {},
   "source": [
    "### Contract: V1_SOP<0-31>_SUBSOP<0-7>\n",
    "- subsop0 - subsop6: adding 100 product terms\n",
    "- subsop7: adding 84 product terms + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63de0f10",
   "metadata": {},
   "source": [
    "#### Contract structure\n",
    "```\n",
    "%lang starknet\n",
    "%builtins pedersen range_check\n",
    "\n",
    "from starkware.cairo.common.cairo_builtins import HashBuiltin\n",
    "from starkware.cairo.common.alloc import alloc\n",
    "\n",
    "@view\n",
    "func compute {\n",
    "        syscall_ptr : felt*, pedersen_ptr : HashBuiltin*, range_check_ptr\n",
    "    }(\n",
    "        x_len : felt,\n",
    "        x : felt*\n",
    "    ) -> (\n",
    "        res : felt\n",
    "    ):\n",
    "    \n",
    "    let res = [x+0]*___ + [x+1]*___ + .... + [x+99]*___\n",
    "    \n",
    "    return (res)\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "f533c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_v1_sop_subsop_contract(sop_idx, subsop_idx, model, quantize_depth = 8):\n",
    "    b_s = model.fc1.bias.tolist()\n",
    "    b = b_s[sop_idx]\n",
    "    w_list = model.fc1.weight[sop_idx].tolist()\n",
    "    f_quantize = lambda e,n : int(e* 10**n)\n",
    "    w_list_quantized = [f_quantize(w, quantize_depth) for w in w_list]\n",
    "    b_quantized = quantize(b, quantize_depth)\n",
    "    ret = []\n",
    "    \n",
    "    ret += [\n",
    "        '%lang starknet',\n",
    "        '%builtins pedersen range_check',\n",
    "        '',\n",
    "        'from starkware.cairo.common.cairo_builtins import HashBuiltin',\n",
    "        ''\n",
    "    ]\n",
    "    \n",
    "    ret += [\n",
    "        '@view',\n",
    "        'func compute {',\n",
    "        '        syscall_ptr : felt*, pedersen_ptr : HashBuiltin*, range_check_ptr',\n",
    "        '    }(',\n",
    "        '        x_len : felt,',\n",
    "        '        x : felt*',\n",
    "        '    ) -> (',\n",
    "        '        res : felt',\n",
    "        '    ):',\n",
    "        ''\n",
    "    ]\n",
    "    if subsop_idx == 7:\n",
    "        ## add 84 product terms and bias\n",
    "        w_list_quantized_ = w_list_quantized[700 : 783]\n",
    "        product_terms = [f'[x+{i}] * {w}' for i,w in enumerate(w_list_quantized_)]\n",
    "        expression = ' + '.join([str(b_quantized)] + product_terms)\n",
    "    else:\n",
    "        ## add 100 product terms\n",
    "        w_list_quantized_ = w_list_quantized[subsop_idx*100 : (subsop_idx+1)*100-1]\n",
    "        product_terms = [f'[x+{i}] * {w}' for i,w in enumerate(w_list_quantized_)]\n",
    "        expression = ' + '.join(product_terms)\n",
    "    ret.append(f'    let res = {expression}')\n",
    "    \n",
    "    ret += [\n",
    "        '    return (res)',\n",
    "        'end'\n",
    "        ''\n",
    "    ]\n",
    "    \n",
    "    with open(f'gen_contract/mnist_v1_sop{sop_idx}_subsop{subsop_idx}.cairo', 'a') as f:\n",
    "        for line in ret:\n",
    "            f.write(line+'\\n')\n",
    "            \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7855a2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Generating 8 subsop contracts for each of the 32 sop's\n"
     ]
    }
   ],
   "source": [
    "print(\"> Generating 8 subsop contracts for each of the 32 sop's\")\n",
    "# for sop_idx in range(32):\n",
    "#     for subsop_idx in range(8):\n",
    "#         ret = gen_v1_sop_subsop_contract(sop_idx, subsop_idx, model, quantize_depth = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b658158",
   "metadata": {},
   "source": [
    "### relu contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "207fdedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Generating cairo code for relu contract\n"
     ]
    }
   ],
   "source": [
    "print(\"> Generating cairo code for relu contract\")\n",
    "# for i in range(32):\n",
    "#     print(f'    let (h) = _relu([v1+{i}])')\n",
    "#     print(f'    assert [h1+{i}] = h')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2627c6",
   "metadata": {},
   "source": [
    "### Z contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e14ebbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Generating cairo code for the z (FC2) contract\n"
     ]
    }
   ],
   "source": [
    "print(\"> Generating cairo code for the Z (FC2) contract\")\n",
    "# for i in range(10):\n",
    "#     print(f'    let (addr_{i}) = stored_addresses.read({i})')\n",
    "#     print(f'    local pedersen_ptr : HashBuiltin* = pedersen_ptr')\n",
    "#     print(f'    let (z_{i}) = IContractZSOP{i}.compute(addr_{i}, 32, h1)')\n",
    "#     print(f'    assert [z+{i}] = z_{i}')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b32966",
   "metadata": {},
   "source": [
    "### z_sop<0-9> contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "e4b1c049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_z_sop_contract(sop_idx, model, quantize_depth = 8):\n",
    "    b_s = model.fc2.bias.tolist()\n",
    "    b = b_s[sop_idx]\n",
    "    w_list = model.fc2.weight[sop_idx].tolist()\n",
    "    f_quantize = lambda e,n : int(e* 10**n)\n",
    "    w_list_quantized = [f_quantize(w, quantize_depth) for w in w_list]\n",
    "    b_quantized = quantize(b, quantize_depth)\n",
    "    ret = []\n",
    "    \n",
    "    ret += [\n",
    "        '%lang starknet',\n",
    "        '%builtins pedersen range_check',\n",
    "        '',\n",
    "        'from starkware.cairo.common.cairo_builtins import HashBuiltin',\n",
    "        ''\n",
    "    ]\n",
    "    \n",
    "    ret += [\n",
    "        '@view',\n",
    "        'func compute {',\n",
    "        '        syscall_ptr : felt*, pedersen_ptr : HashBuiltin*, range_check_ptr',\n",
    "        '    }(',\n",
    "        '        x_len : felt,',\n",
    "        '        x : felt*',\n",
    "        '    ) -> (',\n",
    "        '        res : felt',\n",
    "        '    ):',\n",
    "        ''\n",
    "    ]\n",
    "    \n",
    "    product_terms = [f'[x+{i}] * {w}' for i,w in enumerate(w_list_quantized)]\n",
    "    expression = ' + '.join([str(b_quantized)] + product_terms)\n",
    "    ret.append(f'    let res = {expression}')\n",
    "    \n",
    "    ret += [\n",
    "        '    return (res)',\n",
    "        'end'\n",
    "        ''\n",
    "    ]\n",
    "    \n",
    "    with open(f'gen_contract/mnist_z_sop{sop_idx}.cairo', 'a') as f:\n",
    "        for line in ret:\n",
    "            f.write(line+'\\n')\n",
    "            \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8332264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Generating 10 sop contracts for Z contract\n"
     ]
    }
   ],
   "source": [
    "print(\"> Generating 10 sop contracts for Z contract\")\n",
    "# for i in range(10):\n",
    "#     gen_z_sop_contract(i, model, 8);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b6070",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179a68ee",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "3de47637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args['log_interval'] == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args['dry_run']:\n",
    "                break\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def run():\n",
    "    args = {\n",
    "        'no_cuda' : True,\n",
    "        'seed' : 1,\n",
    "        'batch_size' : 64,\n",
    "        'test_batch_size' : 1000,\n",
    "        'lr' : 1.0,\n",
    "        'gamma' : 0.7,\n",
    "        'epochs' : 14,\n",
    "        'save_model' : True,\n",
    "        'log_interval' : 10,\n",
    "        'dry_run' : False\n",
    "        \n",
    "    }\n",
    "   \n",
    "    use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args['seed'])\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': args['batch_size']}\n",
    "    test_kwargs = {'batch_size': args['test_batch_size']}\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST('../data', train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args['lr'])\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args['gamma'])\n",
    "    for epoch in range(1, args['epochs'] + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args['save_model']:\n",
    "        torch.save(model.state_dict(), \"mnist_dnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f85d4e81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.329856\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.573720\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1.413218\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.955432\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.786503\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.881180\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.771456\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.804304\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.143172\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.575278\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.671376\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.671880\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.625420\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.604873\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.773807\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.766047\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.657894\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.557043\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.810708\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.812729\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.627280\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.509565\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.879667\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.953598\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.594260\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.849802\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.799870\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.444510\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.601406\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.698540\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.652794\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.514471\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.523733\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.781636\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.422628\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.458730\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.632195\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.684939\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.358969\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.511623\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.537560\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.692182\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.752163\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.488838\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.624169\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.651200\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.524830\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.697268\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.602215\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.694968\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.730230\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.615659\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.605882\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.382978\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.511288\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.757788\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.545386\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.469098\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.704659\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.634796\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.537917\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.376806\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.586106\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.576286\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.736057\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.777634\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.401054\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.725106\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.654969\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.493834\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.689018\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.931576\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.666565\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.693394\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.428320\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.409523\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.412265\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.413866\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.419289\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.574890\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.479659\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.504632\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.219473\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.601869\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.262457\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.568556\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.644358\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.452808\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.483250\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.527044\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.862484\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.331764\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.207774\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.281721\n",
      "\n",
      "Test set: Average loss: 0.2707, Accuracy: 9185/10000 (92%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.495284\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.485000\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.659054\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.358853\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.409103\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.443521\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.308301\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.384405\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.704436\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.618382\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.626117\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.418247\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.459344\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.442262\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.380311\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.621462\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.445887\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.460780\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.625149\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.654534\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.346784\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.359561\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.494535\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.867174\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.422587\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.549479\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.629367\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.240606\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.488836\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.502311\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.559503\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.398619\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.413032\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.354131\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.302749\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.476233\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.591439\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.545878\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.304923\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.545585\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.585832\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.649650\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.914222\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.454977\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.486954\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.338866\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.375327\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.529471\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.670390\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.714792\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.542389\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.497328\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.393173\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.415825\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.376243\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.743936\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.532307\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.478154\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.511486\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.550321\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.378240\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.186997\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.578166\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.526799\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.704806\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.470647\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.520895\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.617220\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.640312\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.297285\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.654528\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.742038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.461188\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.600052\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.580971\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.632555\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.320015\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.392818\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.312475\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.463928\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.524607\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.414306\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.191782\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.566535\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.429980\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.474902\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.421944\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.590989\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.486295\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.444422\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.918812\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.260515\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.277935\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.262114\n",
      "\n",
      "Test set: Average loss: 0.2419, Accuracy: 9271/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.259714\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.607980\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.508072\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.318094\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.316845\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.416210\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.213685\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.346675\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.769592\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.355125\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.409745\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.315695\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.254929\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.427996\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.282381\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.480844\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.614786\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.303540\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.773449\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.553182\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.326468\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.387441\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.561663\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.971835\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.310168\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.711797\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.474177\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.228002\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.335945\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.497269\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.574885\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.247792\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.400681\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.325883\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.165342\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.462703\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.549450\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.705275\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.300483\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.412043\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.336228\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.598121\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.576202\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.466966\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.447834\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.252061\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.274659\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.548277\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.489900\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.412327\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.553893\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.453721\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.482098\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.246906\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.419086\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.503929\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.383592\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.476421\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.512338\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.596540\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.337791\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.288810\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.553977\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.455420\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.773653\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.563582\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.338702\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.597800\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.555291\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.388376\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.704398\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.648789\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.396679\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.527533\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.381821\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.300343\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.730483\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.351399\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.384403\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.432392\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.401542\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.399903\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.304475\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.512452\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.265676\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.585391\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.373960\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.535892\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.446592\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.353325\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.773506\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.192338\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.261443\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.145571\n",
      "\n",
      "Test set: Average loss: 0.2263, Accuracy: 9343/10000 (93%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.232320\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.608017\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.664694\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.414493\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.370873\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.476811\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.253435\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.304627\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.898732\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.324280\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.395601\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.335445\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.331736\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.279492\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.322308\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.450102\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.590062\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.362857\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.993682\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.562067\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.356414\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.497555\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.366102\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.758772\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.361953\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.712262\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.444590\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.252942\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.400888\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.356160\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.413044\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.356233\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.371855\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.299513\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.219646\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.551440\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.551008\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.833405\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.299134\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.476929\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.475410\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.522580\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.523845\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.386525\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.382917\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.368260\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.353417\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.633949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.387406\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.387632\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.627937\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.442815\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.556867\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.204338\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.364329\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.449830\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.330669\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.625873\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.464646\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.338473\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.240435\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.320967\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.527491\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.282453\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.605719\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.473271\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.482670\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.489779\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.498007\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.315079\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.453502\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.727448\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.552200\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.498099\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.401602\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.451216\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.435243\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.325070\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.427213\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.666465\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.564493\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.338509\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.438377\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.415633\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.258177\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.399892\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.341231\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.557606\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.368240\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.419358\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.454436\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.308101\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.142753\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.146137\n",
      "\n",
      "Test set: Average loss: 0.2152, Accuracy: 9377/10000 (94%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.327726\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.602816\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.618734\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.406728\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.278367\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.222456\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.213637\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.434184\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.774697\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.455385\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.323970\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.323827\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.296584\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.410509\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.595034\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.539750\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.420412\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.287016\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.788983\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.693811\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.307838\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.503727\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.498015\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.699857\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.356899\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.777951\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.448763\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.158739\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.310376\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.589268\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.525371\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.315737\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.407925\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.455113\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.359307\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.258054\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.525928\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.620767\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.315889\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.467205\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.579081\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.384094\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.920208\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.330741\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.459820\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.277562\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.460491\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.729560\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.461435\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.562131\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.511046\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.340477\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.248162\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.244922\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.407138\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.585182\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.293625\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.312841\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.405301\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.577050\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.340332\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.204533\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.383036\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.424317\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.507311\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.385068\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.520079\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.476042\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.461021\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.360668\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.352809\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.641537\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.396436\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.474984\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.358030\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.328460\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.537747\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.232550\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.546459\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.392212\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.390760\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.329464\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.221542\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.436701\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.368363\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.446160\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.367321\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.552634\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.356216\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.383405\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.635585\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.203285\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.186442\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.231743\n",
      "\n",
      "Test set: Average loss: 0.2060, Accuracy: 9395/10000 (94%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.327386\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.496632\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.572833\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.399891\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.292807\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.452294\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.231679\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.274354\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.758675\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.466718\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.477198\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.410603\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.429338\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.187306\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.418083\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.381455\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.640136\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.454042\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.628522\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.472793\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.347232\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.491486\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.378928\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.680948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.270076\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.802154\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.507357\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.173361\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.285443\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.587554\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.459232\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.277767\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.325778\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.385050\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.282043\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.277236\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.600683\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.505492\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.234278\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.356704\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.398274\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.572113\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.566408\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.472434\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.484317\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.410122\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.384338\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.507482\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.396479\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.388485\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.528514\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.484322\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.401150\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.294727\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.378423\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.547225\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.322257\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.328275\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.517598\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.573544\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.261940\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.219575\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.370018\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.281405\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.667877\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.422005\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.265725\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.521210\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.553423\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.230255\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.595129\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.452917\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.524552\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.471082\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.478730\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.420801\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.417354\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.217358\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.412738\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.410106\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.339937\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.282458\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.173636\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.330321\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.356844\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.422565\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.244715\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.547800\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.418256\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.478962\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.581374\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.217978\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.132403\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.269151\n",
      "\n",
      "Test set: Average loss: 0.2032, Accuracy: 9397/10000 (94%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.298520\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.567183\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.609927\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.451847\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.323477\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.365740\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.283856\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.325188\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 1.093638\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.401033\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.395740\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.254759\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.274237\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.484275\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.388615\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.437045\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.566227\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.413321\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.808007\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.385343\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.281689\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.466586\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.583914\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.629823\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.427076\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.594346\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.417486\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.267016\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.329401\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.500928\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.557418\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.305521\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.336028\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.487529\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.328707\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.366948\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.374221\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.698696\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.196332\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.398969\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.459565\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.542232\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.647708\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.396407\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.432849\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.369074\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.366718\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.528509\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.675764\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.385476\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.371751\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.588095\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.487439\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.250328\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.387349\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.505132\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.369889\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.424885\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.605786\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.385328\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.385328\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.126244\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.511842\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.254933\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.660018\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.331501\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.342936\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.508220\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.387427\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.218554\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.361913\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.724952\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.481175\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.516788\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.403087\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.451004\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.314435\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.174006\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.317626\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.426985\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.588124\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.370901\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.194151\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.421936\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.213649\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.561513\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.300987\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.550283\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.351847\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.275629\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.463186\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.192599\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.144916\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.232804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2007, Accuracy: 9426/10000 (94%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.328883\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.481213\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.532987\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.208293\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.415460\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.406048\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.194627\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.262633\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.625460\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.477707\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.376385\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.176200\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.236408\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.473929\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.286555\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.437849\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.481544\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.350946\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.934134\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.453948\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.286373\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.534674\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.413464\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.839029\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.320756\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.637603\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.536588\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.234514\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.322017\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.458477\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.365328\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.289921\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.233518\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.329823\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.110502\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.405460\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.673468\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.687868\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.195616\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.340281\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.470733\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.420237\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.561275\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.388945\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.397581\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.304728\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.437336\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.568742\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.401218\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.442373\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.635838\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.501577\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.450858\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.326261\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.340863\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.589925\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.422368\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.437841\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.410812\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.454950\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.443415\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.218219\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.484559\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.337386\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.623801\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.321867\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.394571\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.544867\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.353516\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.209187\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.400064\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.797199\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.439380\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.428589\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.387954\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.317376\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.387063\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.199483\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.434929\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.517445\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.477990\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.281699\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.382181\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.241266\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.316405\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.290061\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.375204\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.437426\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.294251\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.268928\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.435749\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.163832\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.121195\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.205260\n",
      "\n",
      "Test set: Average loss: 0.2000, Accuracy: 9427/10000 (94%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.313123\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.275536\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.485800\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.211556\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.322971\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.332719\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.223380\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.291843\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.698274\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.405546\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.421636\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.374821\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.350884\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.203504\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.332502\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.418447\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.432719\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.322410\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.881569\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.533958\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.245916\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.376117\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.485035\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.597886\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.281330\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.720173\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.552473\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.245789\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.244076\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.485165\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.441904\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.285723\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.242586\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.338120\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.203901\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.394555\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.529545\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.567798\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.257282\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.421654\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.290657\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.415053\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.956548\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.364037\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.296019\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.343270\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.446676\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.540542\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.367840\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.315982\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.591831\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.388922\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.318818\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.234004\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.585632\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.406069\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.281554\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.471109\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.598098\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.442606\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.312025\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.230095\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.445186\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.197103\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.482667\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.301394\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.267920\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.506218\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.459571\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.226434\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.431623\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.607390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.358094\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.517971\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.415541\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.276986\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.364222\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.231730\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.334715\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.383083\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.219380\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.254993\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.187745\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.310694\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.294213\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.323858\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.235604\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.561703\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.321078\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.294328\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.488895\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.197956\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.118269\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.155988\n",
      "\n",
      "Test set: Average loss: 0.1990, Accuracy: 9435/10000 (94%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.460573\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.367874\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.458003\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.378055\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.209652\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.315305\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.271764\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.422221\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.548371\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.188131\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.432521\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.340333\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.426434\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.378784\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.363532\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.406029\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.548945\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.296011\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.619379\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.426020\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.402373\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.396627\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.507295\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.559664\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.427775\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.525237\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.288735\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.187546\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.376165\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.403871\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.352795\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.232882\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.313654\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.445687\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.142628\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.255582\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.616811\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.729964\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.304235\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.500738\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.350449\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.516342\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.475526\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.555221\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.316871\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.216529\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.222499\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.492832\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.345449\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.578507\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.637806\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.440823\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.594140\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.223186\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.433629\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.488908\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.255510\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.446813\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.351090\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.424445\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.264361\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.128673\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.337424\n",
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.302589\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.562559\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.356267\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.382943\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.506730\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.378505\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.262243\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.275361\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.603640\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.379584\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.408530\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.422284\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.288029\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.462590\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.286350\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.475472\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.463452\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.411296\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.274192\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.115500\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.360307\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.238671\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.383041\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.344083\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.587084\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.260459\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.278214\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.418198\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.174710\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.127294\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.283945\n",
      "\n",
      "Test set: Average loss: 0.1976, Accuracy: 9435/10000 (94%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.202882\n",
      "Train Epoch: 11 [640/60000 (1%)]\tLoss: 0.574806\n",
      "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 0.315549\n",
      "Train Epoch: 11 [1920/60000 (3%)]\tLoss: 0.233222\n",
      "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.300519\n",
      "Train Epoch: 11 [3200/60000 (5%)]\tLoss: 0.352544\n",
      "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 0.205421\n",
      "Train Epoch: 11 [4480/60000 (7%)]\tLoss: 0.320293\n",
      "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.902290\n",
      "Train Epoch: 11 [5760/60000 (10%)]\tLoss: 0.180149\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.421139\n",
      "Train Epoch: 11 [7040/60000 (12%)]\tLoss: 0.385600\n",
      "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.331473\n",
      "Train Epoch: 11 [8320/60000 (14%)]\tLoss: 0.327596\n",
      "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 0.252347\n",
      "Train Epoch: 11 [9600/60000 (16%)]\tLoss: 0.345660\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.492691\n",
      "Train Epoch: 11 [10880/60000 (18%)]\tLoss: 0.263347\n",
      "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 0.676917\n",
      "Train Epoch: 11 [12160/60000 (20%)]\tLoss: 0.458169\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.313049\n",
      "Train Epoch: 11 [13440/60000 (22%)]\tLoss: 0.453080\n",
      "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 0.529831\n",
      "Train Epoch: 11 [14720/60000 (25%)]\tLoss: 0.480843\n",
      "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.327099\n",
      "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 0.518203\n",
      "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 0.454910\n",
      "Train Epoch: 11 [17280/60000 (29%)]\tLoss: 0.148433\n",
      "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.376185\n",
      "Train Epoch: 11 [18560/60000 (31%)]\tLoss: 0.566518\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.392956\n",
      "Train Epoch: 11 [19840/60000 (33%)]\tLoss: 0.269092\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.346974\n",
      "Train Epoch: 11 [21120/60000 (35%)]\tLoss: 0.509801\n",
      "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 0.244836\n",
      "Train Epoch: 11 [22400/60000 (37%)]\tLoss: 0.238497\n",
      "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.496650\n",
      "Train Epoch: 11 [23680/60000 (39%)]\tLoss: 0.661872\n",
      "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 0.206954\n",
      "Train Epoch: 11 [24960/60000 (42%)]\tLoss: 0.347886\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.214976\n",
      "Train Epoch: 11 [26240/60000 (44%)]\tLoss: 0.427417\n",
      "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 0.559160\n",
      "Train Epoch: 11 [27520/60000 (46%)]\tLoss: 0.322858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.476216\n",
      "Train Epoch: 11 [28800/60000 (48%)]\tLoss: 0.339454\n",
      "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 0.273320\n",
      "Train Epoch: 11 [30080/60000 (50%)]\tLoss: 0.601405\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.410095\n",
      "Train Epoch: 11 [31360/60000 (52%)]\tLoss: 0.578066\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.673122\n",
      "Train Epoch: 11 [32640/60000 (54%)]\tLoss: 0.537931\n",
      "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.490873\n",
      "Train Epoch: 11 [33920/60000 (57%)]\tLoss: 0.246622\n",
      "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 0.420692\n",
      "Train Epoch: 11 [35200/60000 (59%)]\tLoss: 0.400236\n",
      "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.310979\n",
      "Train Epoch: 11 [36480/60000 (61%)]\tLoss: 0.327893\n",
      "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 0.396460\n",
      "Train Epoch: 11 [37760/60000 (63%)]\tLoss: 0.455136\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.220084\n",
      "Train Epoch: 11 [39040/60000 (65%)]\tLoss: 0.356075\n",
      "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 0.585818\n",
      "Train Epoch: 11 [40320/60000 (67%)]\tLoss: 0.265832\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.811314\n",
      "Train Epoch: 11 [41600/60000 (69%)]\tLoss: 0.301214\n",
      "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 0.346182\n",
      "Train Epoch: 11 [42880/60000 (71%)]\tLoss: 0.656145\n",
      "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.473058\n",
      "Train Epoch: 11 [44160/60000 (74%)]\tLoss: 0.243595\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.382388\n",
      "Train Epoch: 11 [45440/60000 (76%)]\tLoss: 0.571935\n",
      "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.445187\n",
      "Train Epoch: 11 [46720/60000 (78%)]\tLoss: 0.535805\n",
      "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 0.382525\n",
      "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 0.254438\n",
      "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.466107\n",
      "Train Epoch: 11 [49280/60000 (82%)]\tLoss: 0.142646\n",
      "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 0.456386\n",
      "Train Epoch: 11 [50560/60000 (84%)]\tLoss: 0.374960\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.276153\n",
      "Train Epoch: 11 [51840/60000 (86%)]\tLoss: 0.437749\n",
      "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 0.067204\n",
      "Train Epoch: 11 [53120/60000 (88%)]\tLoss: 0.361587\n",
      "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 0.167690\n",
      "Train Epoch: 11 [54400/60000 (91%)]\tLoss: 0.211842\n",
      "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 0.295654\n",
      "Train Epoch: 11 [55680/60000 (93%)]\tLoss: 0.522358\n",
      "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.352295\n",
      "Train Epoch: 11 [56960/60000 (95%)]\tLoss: 0.268300\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.568156\n",
      "Train Epoch: 11 [58240/60000 (97%)]\tLoss: 0.243306\n",
      "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.171024\n",
      "Train Epoch: 11 [59520/60000 (99%)]\tLoss: 0.226532\n",
      "\n",
      "Test set: Average loss: 0.1977, Accuracy: 9437/10000 (94%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.324146\n",
      "Train Epoch: 12 [640/60000 (1%)]\tLoss: 0.424487\n",
      "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 0.386360\n",
      "Train Epoch: 12 [1920/60000 (3%)]\tLoss: 0.225062\n",
      "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.374745\n",
      "Train Epoch: 12 [3200/60000 (5%)]\tLoss: 0.329087\n",
      "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 0.171754\n",
      "Train Epoch: 12 [4480/60000 (7%)]\tLoss: 0.373854\n",
      "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.848124\n",
      "Train Epoch: 12 [5760/60000 (10%)]\tLoss: 0.329671\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.406831\n",
      "Train Epoch: 12 [7040/60000 (12%)]\tLoss: 0.212709\n",
      "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.359560\n",
      "Train Epoch: 12 [8320/60000 (14%)]\tLoss: 0.262796\n",
      "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 0.189908\n",
      "Train Epoch: 12 [9600/60000 (16%)]\tLoss: 0.227050\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.485152\n",
      "Train Epoch: 12 [10880/60000 (18%)]\tLoss: 0.250531\n",
      "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 0.630788\n",
      "Train Epoch: 12 [12160/60000 (20%)]\tLoss: 0.751364\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.358753\n",
      "Train Epoch: 12 [13440/60000 (22%)]\tLoss: 0.426296\n",
      "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 0.447329\n",
      "Train Epoch: 12 [14720/60000 (25%)]\tLoss: 0.787671\n",
      "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.319354\n",
      "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 0.487101\n",
      "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 0.443562\n",
      "Train Epoch: 12 [17280/60000 (29%)]\tLoss: 0.232232\n",
      "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.168894\n",
      "Train Epoch: 12 [18560/60000 (31%)]\tLoss: 0.286549\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.538626\n",
      "Train Epoch: 12 [19840/60000 (33%)]\tLoss: 0.259005\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.232334\n",
      "Train Epoch: 12 [21120/60000 (35%)]\tLoss: 0.384388\n",
      "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 0.125807\n",
      "Train Epoch: 12 [22400/60000 (37%)]\tLoss: 0.442925\n",
      "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.441561\n",
      "Train Epoch: 12 [23680/60000 (39%)]\tLoss: 0.797844\n",
      "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 0.259073\n",
      "Train Epoch: 12 [24960/60000 (42%)]\tLoss: 0.345518\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.354123\n",
      "Train Epoch: 12 [26240/60000 (44%)]\tLoss: 0.521159\n",
      "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 0.976633\n",
      "Train Epoch: 12 [27520/60000 (46%)]\tLoss: 0.316981\n",
      "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.209595\n",
      "Train Epoch: 12 [28800/60000 (48%)]\tLoss: 0.279099\n",
      "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 0.344515\n",
      "Train Epoch: 12 [30080/60000 (50%)]\tLoss: 0.641242\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.445317\n",
      "Train Epoch: 12 [31360/60000 (52%)]\tLoss: 0.376564\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.462565\n",
      "Train Epoch: 12 [32640/60000 (54%)]\tLoss: 0.502722\n",
      "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.270088\n",
      "Train Epoch: 12 [33920/60000 (57%)]\tLoss: 0.266488\n",
      "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 0.361246\n",
      "Train Epoch: 12 [35200/60000 (59%)]\tLoss: 0.418240\n",
      "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.334427\n",
      "Train Epoch: 12 [36480/60000 (61%)]\tLoss: 0.388047\n",
      "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 0.399757\n",
      "Train Epoch: 12 [37760/60000 (63%)]\tLoss: 0.409633\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.284212\n",
      "Train Epoch: 12 [39040/60000 (65%)]\tLoss: 0.261967\n",
      "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 0.346965\n",
      "Train Epoch: 12 [40320/60000 (67%)]\tLoss: 0.224053\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.384717\n",
      "Train Epoch: 12 [41600/60000 (69%)]\tLoss: 0.387550\n",
      "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 0.439427\n",
      "Train Epoch: 12 [42880/60000 (71%)]\tLoss: 0.620489\n",
      "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 0.530834\n",
      "Train Epoch: 12 [44160/60000 (74%)]\tLoss: 0.302054\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.482938\n",
      "Train Epoch: 12 [45440/60000 (76%)]\tLoss: 0.652643\n",
      "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.428761\n",
      "Train Epoch: 12 [46720/60000 (78%)]\tLoss: 0.476768\n",
      "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 0.397939\n",
      "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.471802\n",
      "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.511492\n",
      "Train Epoch: 12 [49280/60000 (82%)]\tLoss: 0.262152\n",
      "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 0.364091\n",
      "Train Epoch: 12 [50560/60000 (84%)]\tLoss: 0.323000\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.368079\n",
      "Train Epoch: 12 [51840/60000 (86%)]\tLoss: 0.357367\n",
      "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 0.237619\n",
      "Train Epoch: 12 [53120/60000 (88%)]\tLoss: 0.378812\n",
      "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 0.247742\n",
      "Train Epoch: 12 [54400/60000 (91%)]\tLoss: 0.353814\n",
      "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 0.288725\n",
      "Train Epoch: 12 [55680/60000 (93%)]\tLoss: 0.474021\n",
      "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.350782\n",
      "Train Epoch: 12 [56960/60000 (95%)]\tLoss: 0.262512\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.719339\n",
      "Train Epoch: 12 [58240/60000 (97%)]\tLoss: 0.296959\n",
      "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.175393\n",
      "Train Epoch: 12 [59520/60000 (99%)]\tLoss: 0.158011\n",
      "\n",
      "Test set: Average loss: 0.1973, Accuracy: 9447/10000 (94%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.250929\n",
      "Train Epoch: 13 [640/60000 (1%)]\tLoss: 0.388162\n",
      "Train Epoch: 13 [1280/60000 (2%)]\tLoss: 0.413388\n",
      "Train Epoch: 13 [1920/60000 (3%)]\tLoss: 0.244110\n",
      "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.368619\n",
      "Train Epoch: 13 [3200/60000 (5%)]\tLoss: 0.391520\n",
      "Train Epoch: 13 [3840/60000 (6%)]\tLoss: 0.214375\n",
      "Train Epoch: 13 [4480/60000 (7%)]\tLoss: 0.279774\n",
      "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.723520\n",
      "Train Epoch: 13 [5760/60000 (10%)]\tLoss: 0.439548\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.391319\n",
      "Train Epoch: 13 [7040/60000 (12%)]\tLoss: 0.459680\n",
      "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.338683\n",
      "Train Epoch: 13 [8320/60000 (14%)]\tLoss: 0.407147\n",
      "Train Epoch: 13 [8960/60000 (15%)]\tLoss: 0.386489\n",
      "Train Epoch: 13 [9600/60000 (16%)]\tLoss: 0.416675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.684186\n",
      "Train Epoch: 13 [10880/60000 (18%)]\tLoss: 0.326011\n",
      "Train Epoch: 13 [11520/60000 (19%)]\tLoss: 0.866696\n",
      "Train Epoch: 13 [12160/60000 (20%)]\tLoss: 0.481567\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.307755\n",
      "Train Epoch: 13 [13440/60000 (22%)]\tLoss: 0.239095\n",
      "Train Epoch: 13 [14080/60000 (23%)]\tLoss: 0.552454\n",
      "Train Epoch: 13 [14720/60000 (25%)]\tLoss: 0.648483\n",
      "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.295497\n",
      "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 0.591951\n",
      "Train Epoch: 13 [16640/60000 (28%)]\tLoss: 0.344244\n",
      "Train Epoch: 13 [17280/60000 (29%)]\tLoss: 0.233474\n",
      "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.299240\n",
      "Train Epoch: 13 [18560/60000 (31%)]\tLoss: 0.553337\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.473008\n",
      "Train Epoch: 13 [19840/60000 (33%)]\tLoss: 0.282226\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.263909\n",
      "Train Epoch: 13 [21120/60000 (35%)]\tLoss: 0.302602\n",
      "Train Epoch: 13 [21760/60000 (36%)]\tLoss: 0.150758\n",
      "Train Epoch: 13 [22400/60000 (37%)]\tLoss: 0.291693\n",
      "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.312457\n",
      "Train Epoch: 13 [23680/60000 (39%)]\tLoss: 0.439806\n",
      "Train Epoch: 13 [24320/60000 (41%)]\tLoss: 0.213099\n",
      "Train Epoch: 13 [24960/60000 (42%)]\tLoss: 0.266980\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.250824\n",
      "Train Epoch: 13 [26240/60000 (44%)]\tLoss: 0.384781\n",
      "Train Epoch: 13 [26880/60000 (45%)]\tLoss: 0.714699\n",
      "Train Epoch: 13 [27520/60000 (46%)]\tLoss: 0.370190\n",
      "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.369041\n",
      "Train Epoch: 13 [28800/60000 (48%)]\tLoss: 0.267969\n",
      "Train Epoch: 13 [29440/60000 (49%)]\tLoss: 0.267261\n",
      "Train Epoch: 13 [30080/60000 (50%)]\tLoss: 0.510806\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.500246\n",
      "Train Epoch: 13 [31360/60000 (52%)]\tLoss: 0.344520\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.380394\n",
      "Train Epoch: 13 [32640/60000 (54%)]\tLoss: 0.466936\n",
      "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.472016\n",
      "Train Epoch: 13 [33920/60000 (57%)]\tLoss: 0.223860\n",
      "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 0.443193\n",
      "Train Epoch: 13 [35200/60000 (59%)]\tLoss: 0.533460\n",
      "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.267028\n",
      "Train Epoch: 13 [36480/60000 (61%)]\tLoss: 0.378015\n",
      "Train Epoch: 13 [37120/60000 (62%)]\tLoss: 0.516908\n",
      "Train Epoch: 13 [37760/60000 (63%)]\tLoss: 0.656856\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.344593\n",
      "Train Epoch: 13 [39040/60000 (65%)]\tLoss: 0.196670\n",
      "Train Epoch: 13 [39680/60000 (66%)]\tLoss: 0.379371\n",
      "Train Epoch: 13 [40320/60000 (67%)]\tLoss: 0.325699\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.443483\n",
      "Train Epoch: 13 [41600/60000 (69%)]\tLoss: 0.383189\n",
      "Train Epoch: 13 [42240/60000 (70%)]\tLoss: 0.277473\n",
      "Train Epoch: 13 [42880/60000 (71%)]\tLoss: 0.449994\n",
      "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.402168\n",
      "Train Epoch: 13 [44160/60000 (74%)]\tLoss: 0.295390\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.298988\n",
      "Train Epoch: 13 [45440/60000 (76%)]\tLoss: 0.467423\n",
      "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.390956\n",
      "Train Epoch: 13 [46720/60000 (78%)]\tLoss: 0.527398\n",
      "Train Epoch: 13 [47360/60000 (79%)]\tLoss: 0.366534\n",
      "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 0.177418\n",
      "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.451046\n",
      "Train Epoch: 13 [49280/60000 (82%)]\tLoss: 0.365157\n",
      "Train Epoch: 13 [49920/60000 (83%)]\tLoss: 0.408421\n",
      "Train Epoch: 13 [50560/60000 (84%)]\tLoss: 0.516191\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.429417\n",
      "Train Epoch: 13 [51840/60000 (86%)]\tLoss: 0.391755\n",
      "Train Epoch: 13 [52480/60000 (87%)]\tLoss: 0.189220\n",
      "Train Epoch: 13 [53120/60000 (88%)]\tLoss: 0.371118\n",
      "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 0.276670\n",
      "Train Epoch: 13 [54400/60000 (91%)]\tLoss: 0.279867\n",
      "Train Epoch: 13 [55040/60000 (92%)]\tLoss: 0.288057\n",
      "Train Epoch: 13 [55680/60000 (93%)]\tLoss: 0.560322\n",
      "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 0.343901\n",
      "Train Epoch: 13 [56960/60000 (95%)]\tLoss: 0.321631\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.554447\n",
      "Train Epoch: 13 [58240/60000 (97%)]\tLoss: 0.219539\n",
      "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 0.156561\n",
      "Train Epoch: 13 [59520/60000 (99%)]\tLoss: 0.287201\n",
      "\n",
      "Test set: Average loss: 0.1965, Accuracy: 9443/10000 (94%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.302586\n",
      "Train Epoch: 14 [640/60000 (1%)]\tLoss: 0.425500\n",
      "Train Epoch: 14 [1280/60000 (2%)]\tLoss: 0.539190\n",
      "Train Epoch: 14 [1920/60000 (3%)]\tLoss: 0.468072\n",
      "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.315768\n",
      "Train Epoch: 14 [3200/60000 (5%)]\tLoss: 0.233739\n",
      "Train Epoch: 14 [3840/60000 (6%)]\tLoss: 0.170426\n",
      "Train Epoch: 14 [4480/60000 (7%)]\tLoss: 0.449069\n",
      "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 0.790052\n",
      "Train Epoch: 14 [5760/60000 (10%)]\tLoss: 0.345463\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.489603\n",
      "Train Epoch: 14 [7040/60000 (12%)]\tLoss: 0.321963\n",
      "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 0.223704\n",
      "Train Epoch: 14 [8320/60000 (14%)]\tLoss: 0.450391\n",
      "Train Epoch: 14 [8960/60000 (15%)]\tLoss: 0.325320\n",
      "Train Epoch: 14 [9600/60000 (16%)]\tLoss: 0.526057\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.410825\n",
      "Train Epoch: 14 [10880/60000 (18%)]\tLoss: 0.253708\n",
      "Train Epoch: 14 [11520/60000 (19%)]\tLoss: 0.759924\n",
      "Train Epoch: 14 [12160/60000 (20%)]\tLoss: 0.391159\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.354941\n",
      "Train Epoch: 14 [13440/60000 (22%)]\tLoss: 0.484609\n",
      "Train Epoch: 14 [14080/60000 (23%)]\tLoss: 0.403497\n",
      "Train Epoch: 14 [14720/60000 (25%)]\tLoss: 0.723106\n",
      "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 0.467376\n",
      "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 0.663980\n",
      "Train Epoch: 14 [16640/60000 (28%)]\tLoss: 0.460443\n",
      "Train Epoch: 14 [17280/60000 (29%)]\tLoss: 0.261465\n",
      "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.329590\n",
      "Train Epoch: 14 [18560/60000 (31%)]\tLoss: 0.249152\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.366411\n",
      "Train Epoch: 14 [19840/60000 (33%)]\tLoss: 0.330451\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.387336\n",
      "Train Epoch: 14 [21120/60000 (35%)]\tLoss: 0.500325\n",
      "Train Epoch: 14 [21760/60000 (36%)]\tLoss: 0.272956\n",
      "Train Epoch: 14 [22400/60000 (37%)]\tLoss: 0.354595\n",
      "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.566402\n",
      "Train Epoch: 14 [23680/60000 (39%)]\tLoss: 0.654961\n",
      "Train Epoch: 14 [24320/60000 (41%)]\tLoss: 0.224775\n",
      "Train Epoch: 14 [24960/60000 (42%)]\tLoss: 0.453892\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.557989\n",
      "Train Epoch: 14 [26240/60000 (44%)]\tLoss: 0.431099\n",
      "Train Epoch: 14 [26880/60000 (45%)]\tLoss: 0.650233\n",
      "Train Epoch: 14 [27520/60000 (46%)]\tLoss: 0.334776\n",
      "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 0.607559\n",
      "Train Epoch: 14 [28800/60000 (48%)]\tLoss: 0.386647\n",
      "Train Epoch: 14 [29440/60000 (49%)]\tLoss: 0.364095\n",
      "Train Epoch: 14 [30080/60000 (50%)]\tLoss: 0.551386\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.517428\n",
      "Train Epoch: 14 [31360/60000 (52%)]\tLoss: 0.603128\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.612161\n",
      "Train Epoch: 14 [32640/60000 (54%)]\tLoss: 0.643186\n",
      "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.322964\n",
      "Train Epoch: 14 [33920/60000 (57%)]\tLoss: 0.226013\n",
      "Train Epoch: 14 [34560/60000 (58%)]\tLoss: 0.416041\n",
      "Train Epoch: 14 [35200/60000 (59%)]\tLoss: 0.564287\n",
      "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 0.437571\n",
      "Train Epoch: 14 [36480/60000 (61%)]\tLoss: 0.551221\n",
      "Train Epoch: 14 [37120/60000 (62%)]\tLoss: 0.676804\n",
      "Train Epoch: 14 [37760/60000 (63%)]\tLoss: 0.400236\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.255324\n",
      "Train Epoch: 14 [39040/60000 (65%)]\tLoss: 0.150489\n",
      "Train Epoch: 14 [39680/60000 (66%)]\tLoss: 0.371381\n",
      "Train Epoch: 14 [40320/60000 (67%)]\tLoss: 0.253574\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.575552\n",
      "Train Epoch: 14 [41600/60000 (69%)]\tLoss: 0.297371\n",
      "Train Epoch: 14 [42240/60000 (70%)]\tLoss: 0.415579\n",
      "Train Epoch: 14 [42880/60000 (71%)]\tLoss: 0.631773\n",
      "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 0.393798\n",
      "Train Epoch: 14 [44160/60000 (74%)]\tLoss: 0.310301\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.427006\n",
      "Train Epoch: 14 [45440/60000 (76%)]\tLoss: 0.591965\n",
      "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.618065\n",
      "Train Epoch: 14 [46720/60000 (78%)]\tLoss: 0.532020\n",
      "Train Epoch: 14 [47360/60000 (79%)]\tLoss: 0.330531\n",
      "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 0.348338\n",
      "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.439089\n",
      "Train Epoch: 14 [49280/60000 (82%)]\tLoss: 0.260796\n",
      "Train Epoch: 14 [49920/60000 (83%)]\tLoss: 0.403103\n",
      "Train Epoch: 14 [50560/60000 (84%)]\tLoss: 0.403478\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.271792\n",
      "Train Epoch: 14 [51840/60000 (86%)]\tLoss: 0.261584\n",
      "Train Epoch: 14 [52480/60000 (87%)]\tLoss: 0.230972\n",
      "Train Epoch: 14 [53120/60000 (88%)]\tLoss: 0.311670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 0.238084\n",
      "Train Epoch: 14 [54400/60000 (91%)]\tLoss: 0.382641\n",
      "Train Epoch: 14 [55040/60000 (92%)]\tLoss: 0.320666\n",
      "Train Epoch: 14 [55680/60000 (93%)]\tLoss: 0.610193\n",
      "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.375267\n",
      "Train Epoch: 14 [56960/60000 (95%)]\tLoss: 0.226622\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.633872\n",
      "Train Epoch: 14 [58240/60000 (97%)]\tLoss: 0.241344\n",
      "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.149917\n",
      "Train Epoch: 14 [59520/60000 (99%)]\tLoss: 0.158832\n",
      "\n",
      "Test set: Average loss: 0.1959, Accuracy: 9436/10000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06892b04",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37b450b",
   "metadata": {},
   "source": [
    "### Datastructure exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2d5bc586",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_s = model.fc1.bias.tolist()\n",
    "b_0 = b_s[0]\n",
    "w_0_list = model.fc1.weight[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "12db393a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.03493674099445343"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c0921083",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.02740933559834957,\n",
       " -0.006756555289030075,\n",
       " 0.0020833066664636135,\n",
       " 0.02577025257050991,\n",
       " -0.024615662172436714,\n",
       " 0.03042559139430523,\n",
       " 0.001659609959460795,\n",
       " 0.027176501229405403,\n",
       " 0.01397180836647749,\n",
       " 0.0046339379623532295,\n",
       " 0.018912842497229576,\n",
       " 0.010768760927021503,\n",
       " 0.01650378294289112,\n",
       " -0.011312826536595821,\n",
       " 0.006403053179383278,\n",
       " 0.005791680887341499,\n",
       " 0.01418349239975214,\n",
       " 0.008864249102771282,\n",
       " 0.04022698849439621,\n",
       " 0.02012101747095585,\n",
       " -0.004293275065720081,\n",
       " -0.012563132680952549,\n",
       " 0.003020653733983636,\n",
       " -0.006398078054189682,\n",
       " -0.0024376793298870325,\n",
       " 0.010716964490711689,\n",
       " 0.03029737062752247,\n",
       " 0.028419116511940956,\n",
       " -0.025905447080731392,\n",
       " 0.03114704042673111,\n",
       " 0.01898438297212124,\n",
       " 0.0428837388753891,\n",
       " 0.032580479979515076,\n",
       " -0.02377472072839737,\n",
       " -0.04583946242928505,\n",
       " -0.03219585120677948,\n",
       " 0.019051160663366318,\n",
       " -0.01690705679357052,\n",
       " 0.0018371023470535874,\n",
       " -0.02316276542842388,\n",
       " 0.012664335779845715,\n",
       " -0.04223133996129036,\n",
       " 0.021850770339369774,\n",
       " -0.018963923677802086,\n",
       " -0.040716491639614105,\n",
       " 0.007981794886291027,\n",
       " -0.017635824158787727,\n",
       " 0.01318256463855505,\n",
       " -0.0014902636175975204,\n",
       " 0.005643315613269806,\n",
       " -0.02700428105890751,\n",
       " -0.009991849772632122,\n",
       " 0.028386877849698067,\n",
       " 0.0003256114141549915,\n",
       " 0.044579967856407166,\n",
       " 0.037634894251823425,\n",
       " 0.007334664463996887,\n",
       " -0.014831704087555408,\n",
       " 0.030755765736103058,\n",
       " 0.020091267302632332,\n",
       " -0.014079430140554905,\n",
       " 0.03125082328915596,\n",
       " 0.011193717829883099,\n",
       " 0.014965307898819447,\n",
       " -0.037842895835638046,\n",
       " -0.04810555279254913,\n",
       " -0.06489609181880951,\n",
       " -0.058012284338474274,\n",
       " -0.11575821042060852,\n",
       " -0.11495702713727951,\n",
       " -0.038023483008146286,\n",
       " -0.04508918523788452,\n",
       " -0.02503271773457527,\n",
       " 0.0005955614033155143,\n",
       " 0.04010416194796562,\n",
       " 0.05535823106765747,\n",
       " 0.050690412521362305,\n",
       " 0.028519051149487495,\n",
       " -0.029398882761597633,\n",
       " 0.018008651211857796,\n",
       " -0.0168903935700655,\n",
       " -0.01793966256082058,\n",
       " 0.011178857646882534,\n",
       " 0.0029209519270807505,\n",
       " 0.029982952401041985,\n",
       " -0.011676483787596226,\n",
       " -0.022742250934243202,\n",
       " 0.03524378687143326,\n",
       " 0.007065195590257645,\n",
       " 0.02323342300951481,\n",
       " -0.010516973212361336,\n",
       " -0.06388293951749802,\n",
       " -0.047862619161605835,\n",
       " -0.08577950298786163,\n",
       " -0.1325766146183014,\n",
       " -0.11723342537879944,\n",
       " -0.14129197597503662,\n",
       " -0.16941168904304504,\n",
       " -0.17074845731258392,\n",
       " -0.11465051025152206,\n",
       " 0.017730576917529106,\n",
       " -0.009906974621117115,\n",
       " 0.06109938398003578,\n",
       " 0.0885043516755104,\n",
       " 0.01463322713971138,\n",
       " -0.006943684071302414,\n",
       " 0.05220003426074982,\n",
       " 0.005211755633354187,\n",
       " 0.030851593241095543,\n",
       " 0.016541576012969017,\n",
       " 0.02049359865486622,\n",
       " 0.035683855414390564,\n",
       " 0.04287157207727432,\n",
       " -0.014692304655909538,\n",
       " 0.013472252525389194,\n",
       " 0.0378573015332222,\n",
       " 0.04192055016756058,\n",
       " 0.04464562237262726,\n",
       " -0.03699556365609169,\n",
       " -0.0938037633895874,\n",
       " -0.12407917529344559,\n",
       " -0.11070489883422852,\n",
       " -0.09428714215755463,\n",
       " -0.1458357870578766,\n",
       " -0.13153870403766632,\n",
       " -0.1806623637676239,\n",
       " -0.1755894124507904,\n",
       " -0.08749310672283173,\n",
       " -0.0375249981880188,\n",
       " 0.027008019387722015,\n",
       " 0.0075994208455085754,\n",
       " 0.07362887263298035,\n",
       " 0.05068541690707207,\n",
       " 0.06884026527404785,\n",
       " 0.07157670706510544,\n",
       " 0.10582539439201355,\n",
       " 0.07179681211709976,\n",
       " 0.026960769668221474,\n",
       " -0.02549242042005062,\n",
       " -0.023297324776649475,\n",
       " -0.007358072325587273,\n",
       " 0.004848163574934006,\n",
       " -0.013050188310444355,\n",
       " 0.027339952066540718,\n",
       " 0.018204255029559135,\n",
       " -0.0016232169000431895,\n",
       " -0.010635432787239552,\n",
       " -0.06039276719093323,\n",
       " -0.006865055300295353,\n",
       " 0.024949030950665474,\n",
       " -0.0023196814581751823,\n",
       " 0.031195178627967834,\n",
       " -0.021395424380898476,\n",
       " -0.005468832328915596,\n",
       " 0.005136610008776188,\n",
       " -0.06709113717079163,\n",
       " -0.0544748418033123,\n",
       " -0.0654100552201271,\n",
       " 0.04796583205461502,\n",
       " 0.05001216009259224,\n",
       " 0.04748732969164848,\n",
       " 0.11421596258878708,\n",
       " 0.1349843144416809,\n",
       " 0.17644895613193512,\n",
       " 0.08961991965770721,\n",
       " 0.061914220452308655,\n",
       " 0.0029741134494543076,\n",
       " 0.0010992406168952584,\n",
       " -0.019454291090369225,\n",
       " -0.007262500002980232,\n",
       " 0.001727627357468009,\n",
       " 0.024153806269168854,\n",
       " 0.017309168353676796,\n",
       " 0.01804766058921814,\n",
       " 0.009468946605920792,\n",
       " 0.041111595928668976,\n",
       " 0.11027874052524567,\n",
       " 0.1133602112531662,\n",
       " 0.033930882811546326,\n",
       " 0.06451766937971115,\n",
       " -0.010814751498401165,\n",
       " 0.03615608811378479,\n",
       " 0.020558780059218407,\n",
       " -0.026604870334267616,\n",
       " -0.016699906438589096,\n",
       " 0.07439132034778595,\n",
       " 0.003139396198093891,\n",
       " -0.05872844159603119,\n",
       " -0.0023808320984244347,\n",
       " 0.04516579955816269,\n",
       " 0.13608942925930023,\n",
       " 0.08730003237724304,\n",
       " 0.07793312519788742,\n",
       " 0.04804651439189911,\n",
       " 0.044952213764190674,\n",
       " -0.019750410690903664,\n",
       " 0.025778887793421745,\n",
       " 0.03424004465341568,\n",
       " 0.058809176087379456,\n",
       " -0.0011387256672605872,\n",
       " -0.0137098990380764,\n",
       " -0.034105610102415085,\n",
       " 0.009111053310334682,\n",
       " -0.005179411731660366,\n",
       " 0.05580953508615494,\n",
       " 0.058725759387016296,\n",
       " 0.05035360902547836,\n",
       " 0.03487052395939827,\n",
       " 0.06365563720464706,\n",
       " 0.07721018046140671,\n",
       " -0.01034024078398943,\n",
       " 0.0016350930090993643,\n",
       " 0.014906736090779305,\n",
       " 0.09209845215082169,\n",
       " 0.047586195170879364,\n",
       " 0.051065824925899506,\n",
       " 0.01175754051655531,\n",
       " -0.03263271227478981,\n",
       " 0.030595969408750534,\n",
       " 0.04952159896492958,\n",
       " 8.38280247990042e-05,\n",
       " 0.08268877118825912,\n",
       " 0.011645058169960976,\n",
       " -0.01722622849047184,\n",
       " 0.026578674092888832,\n",
       " -0.012018345296382904,\n",
       " 0.022487910464406013,\n",
       " 0.05341295897960663,\n",
       " 0.03372689336538315,\n",
       " 0.053264375776052475,\n",
       " -0.0005817437777295709,\n",
       " -0.011434126645326614,\n",
       " 0.051292095333337784,\n",
       " 0.01948445662856102,\n",
       " 0.02025522291660309,\n",
       " 0.06218094378709793,\n",
       " 0.028989257290959358,\n",
       " 0.02736845426261425,\n",
       " 0.01812080480158329,\n",
       " -0.020930007100105286,\n",
       " -0.002011175500229001,\n",
       " 0.03374352306127548,\n",
       " 0.03390553221106529,\n",
       " -0.009355872869491577,\n",
       " -0.060982584953308105,\n",
       " -0.010639781132340431,\n",
       " -0.046099286526441574,\n",
       " -0.031249336898326874,\n",
       " 0.03387922793626785,\n",
       " 0.10026838630437851,\n",
       " 0.08333022892475128,\n",
       " -0.005741330794990063,\n",
       " 0.010600156150758266,\n",
       " 0.031442612409591675,\n",
       " 0.04823010414838791,\n",
       " 0.027085362002253532,\n",
       " 0.03524867445230484,\n",
       " 0.06744375079870224,\n",
       " 0.03340678662061691,\n",
       " -0.011293061077594757,\n",
       " 0.028644289821386337,\n",
       " 0.01748831756412983,\n",
       " -0.030583618208765984,\n",
       " 0.05610913783311844,\n",
       " 0.02920401096343994,\n",
       " -0.017082534730434418,\n",
       " 0.010818921029567719,\n",
       " 0.01628090813755989,\n",
       " 0.03684677928686142,\n",
       " 0.008191663771867752,\n",
       " 0.015438668429851532,\n",
       " 0.02886534482240677,\n",
       " -0.008598755113780499,\n",
       " 0.026066895574331284,\n",
       " -0.055013373494148254,\n",
       " -0.08770427852869034,\n",
       " -0.0021352756302803755,\n",
       " 0.046361684799194336,\n",
       " -0.01856282725930214,\n",
       " 0.009910404682159424,\n",
       " 0.0016856216825544834,\n",
       " 0.02829115092754364,\n",
       " 0.05269543454051018,\n",
       " 0.04898054152727127,\n",
       " 0.008432946167886257,\n",
       " 0.032993849366903305,\n",
       " -0.013606294989585876,\n",
       " -0.0310567244887352,\n",
       " -0.021828293800354004,\n",
       " 0.03173545375466347,\n",
       " 0.013301176019012928,\n",
       " 0.0044303047470748425,\n",
       " 0.05719098448753357,\n",
       " 0.0184068251401186,\n",
       " -0.055521950125694275,\n",
       " 0.008055584505200386,\n",
       " 0.024638671427965164,\n",
       " 0.007840300910174847,\n",
       " -0.036395300179719925,\n",
       " -0.030558640137314796,\n",
       " -0.0030727218836545944,\n",
       " -0.054235175251960754,\n",
       " -0.061934538185596466,\n",
       " -0.12443644553422928,\n",
       " -0.07881452143192291,\n",
       " 0.021218713372945786,\n",
       " -0.034562766551971436,\n",
       " 0.04266156256198883,\n",
       " 0.015949318185448647,\n",
       " 0.04298859462141991,\n",
       " 0.017254671081900597,\n",
       " 0.04319903999567032,\n",
       " 0.004625071305781603,\n",
       " 0.02342946268618107,\n",
       " 0.02696801722049713,\n",
       " -0.05628373101353645,\n",
       " 0.02353464625775814,\n",
       " 0.0545739009976387,\n",
       " 0.04644810035824776,\n",
       " 0.0960480347275734,\n",
       " 0.07925761491060257,\n",
       " -0.04439873248338699,\n",
       " -0.007852235808968544,\n",
       " 0.06879928708076477,\n",
       " 0.07973942160606384,\n",
       " 0.04769115522503853,\n",
       " 0.025813965126872063,\n",
       " -0.018737653270363808,\n",
       " -0.02251012809574604,\n",
       " -0.0034027809742838144,\n",
       " -0.032306838780641556,\n",
       " -0.0983276292681694,\n",
       " -0.057621993124485016,\n",
       " -0.029571589082479477,\n",
       " 0.0058452836237847805,\n",
       " 0.0025268278550356627,\n",
       " -0.010963549837470055,\n",
       " -0.0048440503887832165,\n",
       " 0.03178233280777931,\n",
       " 0.06413261592388153,\n",
       " 0.06775807589292526,\n",
       " -0.023357560858130455,\n",
       " 0.004660533741116524,\n",
       " 0.08798668533563614,\n",
       " 0.03478507697582245,\n",
       " 0.03387772664427757,\n",
       " 0.06715695559978485,\n",
       " 0.11375825852155685,\n",
       " -0.008638683706521988,\n",
       " -0.05352957174181938,\n",
       " 0.019636934623122215,\n",
       " 0.07569420337677002,\n",
       " 0.05592617765069008,\n",
       " 0.049068063497543335,\n",
       " 0.06084456667304039,\n",
       " -0.019139211624860764,\n",
       " 0.01788589544594288,\n",
       " 0.002357663819566369,\n",
       " 0.02306409552693367,\n",
       " -0.08603188395500183,\n",
       " -0.08835573494434357,\n",
       " -0.09174097329378128,\n",
       " -0.011548105627298355,\n",
       " 0.003588884836062789,\n",
       " 0.023680152371525764,\n",
       " 0.012486199848353863,\n",
       " -0.017710398882627487,\n",
       " 0.023651259019970894,\n",
       " 0.1340012550354004,\n",
       " 0.08060285449028015,\n",
       " 0.0013316195691004395,\n",
       " 0.10016964375972748,\n",
       " 0.08399944752454758,\n",
       " -0.008809641934931278,\n",
       " 0.03157384321093559,\n",
       " 0.053563885390758514,\n",
       " -0.04698144271969795,\n",
       " -0.019610004499554634,\n",
       " 0.05586733669042587,\n",
       " 0.06268603354692459,\n",
       " 0.06338077783584595,\n",
       " 0.044662851840257645,\n",
       " 0.0595359280705452,\n",
       " 0.000505288306158036,\n",
       " 0.07603757083415985,\n",
       " 0.08644096553325653,\n",
       " 0.005660210736095905,\n",
       " -0.12016960978507996,\n",
       " -0.13214987516403198,\n",
       " -0.09374325722455978,\n",
       " -0.04901478439569473,\n",
       " 0.03438141569495201,\n",
       " 0.014126905240118504,\n",
       " -0.0069998279213905334,\n",
       " 0.047030236572027206,\n",
       " 0.04432423412799835,\n",
       " 0.17260238528251648,\n",
       " 0.08298283815383911,\n",
       " 0.09553347527980804,\n",
       " 0.11780747771263123,\n",
       " 0.0041752043180167675,\n",
       " 0.03224262595176697,\n",
       " -0.00353290350176394,\n",
       " -0.06283245235681534,\n",
       " -0.044098105281591415,\n",
       " -0.058536235243082047,\n",
       " -0.006845559924840927,\n",
       " 0.012738105840981007,\n",
       " 0.06681384891271591,\n",
       " 0.056382425129413605,\n",
       " 0.060769058763980865,\n",
       " 0.10089592635631561,\n",
       " 0.09933901578187943,\n",
       " 0.0038640545681118965,\n",
       " 0.040437113493680954,\n",
       " -0.10873287916183472,\n",
       " -0.17700885236263275,\n",
       " -0.09060719609260559,\n",
       " -0.06855056434869766,\n",
       " -0.006451062858104706,\n",
       " -0.02037818543612957,\n",
       " 0.04164877533912659,\n",
       " 0.006713231094181538,\n",
       " 0.0651007741689682,\n",
       " 0.08795680105686188,\n",
       " 0.039185985922813416,\n",
       " 0.0430561900138855,\n",
       " 0.004404110834002495,\n",
       " 0.02972615882754326,\n",
       " 0.00956125371158123,\n",
       " 0.04866016283631325,\n",
       " 0.0723886638879776,\n",
       " 0.0009133904823102057,\n",
       " -0.04349589720368385,\n",
       " -0.044769492000341415,\n",
       " -0.008624816313385963,\n",
       " 0.02474302425980568,\n",
       " 0.1055859848856926,\n",
       " 0.0687079057097435,\n",
       " 0.07226717472076416,\n",
       " -0.06086040660738945,\n",
       " 0.11866028606891632,\n",
       " 0.08604834228754044,\n",
       " -0.03568544238805771,\n",
       " -0.2149394005537033,\n",
       " -0.1405598223209381,\n",
       " -0.055661458522081375,\n",
       " 0.029097942635416985,\n",
       " -0.008491062559187412,\n",
       " -0.0225606020539999,\n",
       " 0.014248564839363098,\n",
       " 0.009361868724226952,\n",
       " 0.04571470990777016,\n",
       " 0.009412773884832859,\n",
       " 0.04505052790045738,\n",
       " 0.05560091882944107,\n",
       " 0.08696861565113068,\n",
       " 0.014134091325104237,\n",
       " 0.0416412279009819,\n",
       " 0.022554639726877213,\n",
       " 0.07500605285167694,\n",
       " -0.021257558837532997,\n",
       " -0.03292132541537285,\n",
       " 0.05043555423617363,\n",
       " 0.005599028896540403,\n",
       " 0.07727620750665665,\n",
       " 0.02396381087601185,\n",
       " -0.013108693063259125,\n",
       " 0.0032291768584400415,\n",
       " 0.0722772628068924,\n",
       " 0.013232508674263954,\n",
       " -0.12992173433303833,\n",
       " -0.1515125036239624,\n",
       " -0.10897181183099747,\n",
       " -0.021090278401970863,\n",
       " 0.035832278430461884,\n",
       " -0.025196747854351997,\n",
       " 0.02095465175807476,\n",
       " 0.017529431730508804,\n",
       " -0.034055013209581375,\n",
       " -0.030830705538392067,\n",
       " -0.04968990013003349,\n",
       " -0.06983304023742676,\n",
       " -0.004850851837545633,\n",
       " 0.06704437732696533,\n",
       " -0.0046751718036830425,\n",
       " -0.0029764024075120687,\n",
       " 0.016444586217403412,\n",
       " -0.014396845363080502,\n",
       " 0.040658436715602875,\n",
       " -0.01138108316808939,\n",
       " 0.07615310698747635,\n",
       " 0.08350270986557007,\n",
       " 0.059681642800569534,\n",
       " 0.05801061540842056,\n",
       " -0.0041846116073429585,\n",
       " 0.022937126457691193,\n",
       " 0.048913344740867615,\n",
       " -0.08576714247465134,\n",
       " -0.1114964559674263,\n",
       " -0.2084987908601761,\n",
       " -0.0891265943646431,\n",
       " -0.06923123449087143,\n",
       " -0.022946562618017197,\n",
       " 0.002434348687529564,\n",
       " -0.012136310338973999,\n",
       " 0.0015442618168890476,\n",
       " -0.010889066383242607,\n",
       " -0.022430432960391045,\n",
       " -0.1575392782688141,\n",
       " -0.21394842863082886,\n",
       " -0.11782416701316833,\n",
       " -0.14351394772529602,\n",
       " -0.06311525404453278,\n",
       " -0.04282724857330322,\n",
       " -0.06923194229602814,\n",
       " -0.11980056762695312,\n",
       " -0.04026408493518829,\n",
       " -0.004064399283379316,\n",
       " 0.04907694086432457,\n",
       " 0.012644339352846146,\n",
       " -0.0010321680456399918,\n",
       " -0.04772054776549339,\n",
       " -0.059908974915742874,\n",
       " -0.0218732301145792,\n",
       " -0.002411127323284745,\n",
       " -0.12848365306854248,\n",
       " -0.14908458292484283,\n",
       " -0.16561265289783478,\n",
       " -0.05180467292666435,\n",
       " 0.013308758847415447,\n",
       " 0.018296116963028908,\n",
       " 0.02485925517976284,\n",
       " 0.01803440973162651,\n",
       " 0.019638730213046074,\n",
       " 0.015444229356944561,\n",
       " 0.02478678710758686,\n",
       " -0.1234351247549057,\n",
       " -0.2497754991054535,\n",
       " -0.24561282992362976,\n",
       " -0.26977741718292236,\n",
       " -0.29626771807670593,\n",
       " -0.24647772312164307,\n",
       " -0.21567289531230927,\n",
       " -0.22294117510318756,\n",
       " -0.07730244845151901,\n",
       " -0.08434398472309113,\n",
       " -0.05548270046710968,\n",
       " -0.0870368704199791,\n",
       " 0.009325416758656502,\n",
       " -0.0699458122253418,\n",
       " -0.0036099401768296957,\n",
       " -0.006590329576283693,\n",
       " -0.037949759513139725,\n",
       " -0.19234953820705414,\n",
       " -0.17105358839035034,\n",
       " -0.11511855572462082,\n",
       " -0.03553229197859764,\n",
       " -0.0195737536996603,\n",
       " -0.013954843394458294,\n",
       " -0.006738657131791115,\n",
       " -0.0061660632491111755,\n",
       " 0.011943573132157326,\n",
       " 0.023326028138399124,\n",
       " 0.034170880913734436,\n",
       " -0.05707518383860588,\n",
       " -0.17642569541931152,\n",
       " -0.2398415207862854,\n",
       " -0.3132072985172272,\n",
       " -0.30531811714172363,\n",
       " -0.21890804171562195,\n",
       " -0.20024976134300232,\n",
       " -0.1939157247543335,\n",
       " -0.1363040655851364,\n",
       " -0.1387501209974289,\n",
       " -0.13312000036239624,\n",
       " -0.07376868277788162,\n",
       " -0.03554569184780121,\n",
       " -0.08312930166721344,\n",
       " -0.09415579587221146,\n",
       " -0.02227425016462803,\n",
       " -0.014225572347640991,\n",
       " -0.11807283014059067,\n",
       " -0.0972631648182869,\n",
       " -0.060166530311107635,\n",
       " -0.020216939970850945,\n",
       " -0.01840943656861782,\n",
       " 0.03135780617594719,\n",
       " 0.025318531319499016,\n",
       " 0.034904927015304565,\n",
       " -0.005448405165225267,\n",
       " 0.041285645216703415,\n",
       " 0.019423535093665123,\n",
       " -0.02836143784224987,\n",
       " -0.11097479611635208,\n",
       " -0.07565608620643616,\n",
       " -0.04257553815841675,\n",
       " -0.03761414811015129,\n",
       " -0.04128274694085121,\n",
       " -0.04309120774269104,\n",
       " -0.035338204354047775,\n",
       " -0.07207171618938446,\n",
       " -0.08651482313871384,\n",
       " -0.08331138640642166,\n",
       " -0.023144075646996498,\n",
       " -0.09129449725151062,\n",
       " -0.05937200412154198,\n",
       " -0.0029458918143063784,\n",
       " -0.07413674890995026,\n",
       " 0.005821895319968462,\n",
       " -0.08351094275712967,\n",
       " -0.0772365853190422,\n",
       " -0.02973383292555809,\n",
       " 0.012149794958531857,\n",
       " 0.00780763104557991,\n",
       " 0.033128559589385986,\n",
       " 0.022081153467297554,\n",
       " 0.02518453449010849,\n",
       " 0.014926637522876263,\n",
       " -0.0037117137107998133,\n",
       " -0.04890801012516022,\n",
       " -0.12990373373031616,\n",
       " -0.08623462170362473,\n",
       " -0.033178672194480896,\n",
       " 0.09884519875049591,\n",
       " 0.08103130757808685,\n",
       " 0.09762295335531235,\n",
       " 0.07184891402721405,\n",
       " -0.0005754436715506017,\n",
       " -0.02132866531610489,\n",
       " -0.02026800997555256,\n",
       " -0.05768760293722153,\n",
       " 0.002717169001698494,\n",
       " -0.04367896169424057,\n",
       " -0.07248911261558533,\n",
       " -0.033490054309368134,\n",
       " -0.04516587778925896,\n",
       " -0.03949250280857086,\n",
       " -0.06409835070371628,\n",
       " -0.05961741507053375,\n",
       " -0.037229858338832855,\n",
       " -0.018032131716609,\n",
       " 0.033117860555648804,\n",
       " 0.03803730756044388,\n",
       " 0.03175918757915497,\n",
       " 0.024813560768961906,\n",
       " -0.033322013914585114,\n",
       " 0.015548947267234325,\n",
       " -0.08789019286632538,\n",
       " -0.11194080859422684,\n",
       " -0.06476607918739319,\n",
       " 0.032209739089012146,\n",
       " 0.010205566883087158,\n",
       " 0.027782177552580833,\n",
       " 0.06786557286977768,\n",
       " 0.016018597409129143,\n",
       " -0.011985103599727154,\n",
       " 0.02240857109427452,\n",
       " -0.011474652215838432,\n",
       " -0.0806204229593277,\n",
       " -0.06452029198408127,\n",
       " -0.007819236256182194,\n",
       " -0.07655256241559982,\n",
       " -0.09539750963449478,\n",
       " -0.010382365435361862,\n",
       " -0.022152211517095566,\n",
       " 0.020186632871627808,\n",
       " -0.04957395792007446,\n",
       " -0.05309437960386276,\n",
       " -0.025904562324285507,\n",
       " 0.02415022999048233,\n",
       " 0.0050017014145851135,\n",
       " -0.024079320952296257,\n",
       " 0.01562904380261898,\n",
       " 0.002259336644783616,\n",
       " 0.022200237959623337,\n",
       " -0.03240188583731651,\n",
       " 0.002006544964388013,\n",
       " 0.10419503599405289,\n",
       " 0.10761326551437378,\n",
       " 0.12941357493400574,\n",
       " 0.028288304805755615,\n",
       " -0.03513973206281662,\n",
       " 0.0001797826262190938,\n",
       " -0.02823651023209095,\n",
       " -0.061254341155290604,\n",
       " -0.037103768438100815,\n",
       " -0.05419813096523285,\n",
       " -0.071212537586689,\n",
       " -0.08183752745389938,\n",
       " -0.09659793227910995,\n",
       " -0.11201421916484833,\n",
       " -0.009736045263707638,\n",
       " 0.03359851986169815,\n",
       " -0.011818427592515945,\n",
       " -0.02651713229715824,\n",
       " -0.0007603681879118085,\n",
       " -0.018816089257597923,\n",
       " 0.02949426881968975,\n",
       " 0.03414669632911682,\n",
       " -0.013770575635135174,\n",
       " -0.011813520453870296,\n",
       " -0.0043839034624397755,\n",
       " 0.02220299281179905,\n",
       " -0.015367541462182999,\n",
       " 0.04867085814476013,\n",
       " 0.14953042566776276,\n",
       " 0.22215352952480316,\n",
       " 0.27271413803100586,\n",
       " 0.12597283720970154,\n",
       " 0.05959513038396835,\n",
       " 0.08602911233901978,\n",
       " 0.10170026868581772,\n",
       " 0.04660485312342644,\n",
       " 0.02827131748199463,\n",
       " 0.04508734866976738,\n",
       " 0.02547026239335537,\n",
       " 0.027955954894423485,\n",
       " 0.05212900787591934,\n",
       " 0.047333985567092896,\n",
       " 0.124285027384758,\n",
       " 0.07915187627077103,\n",
       " 0.04868674278259277,\n",
       " -0.005395927932113409,\n",
       " 0.02839731238782406,\n",
       " 0.03197087347507477,\n",
       " 0.014138096012175083,\n",
       " -0.004798154346644878,\n",
       " 0.011402164585888386,\n",
       " -0.004713024012744427,\n",
       " 0.04263012856245041,\n",
       " 0.04397760331630707,\n",
       " 0.020651046186685562,\n",
       " 0.02515374682843685,\n",
       " 0.112068310379982,\n",
       " 0.13420408964157104,\n",
       " 0.16334307193756104,\n",
       " 0.15831276774406433,\n",
       " 0.19696258008480072,\n",
       " 0.16800329089164734,\n",
       " 0.15064512193202972,\n",
       " 0.18622417747974396,\n",
       " 0.27984529733657837,\n",
       " 0.23928026854991913,\n",
       " 0.1305919736623764,\n",
       " 0.10949087888002396,\n",
       " 0.0811062902212143,\n",
       " 0.0739482045173645,\n",
       " 0.06823860108852386,\n",
       " 0.019233832135796547,\n",
       " 0.038007255643606186,\n",
       " 0.038969896733760834,\n",
       " 0.027692770585417747,\n",
       " -0.016659962013363838,\n",
       " -0.003457617247477174,\n",
       " 0.037300243973731995,\n",
       " 0.03623446077108383,\n",
       " 0.03179548680782318,\n",
       " -0.005920209921896458,\n",
       " 0.001933786552399397,\n",
       " -0.012924754060804844,\n",
       " -0.006973469629883766,\n",
       " 0.045215897262096405,\n",
       " -0.013052267953753471,\n",
       " 0.00792970135807991,\n",
       " 0.02339288778603077,\n",
       " -0.0006891609518788755,\n",
       " 0.018174367025494576,\n",
       " 0.016490239650011063,\n",
       " 0.04766969010233879,\n",
       " 0.006602147594094276,\n",
       " 0.029368732124567032,\n",
       " -0.0074187396094202995,\n",
       " 0.04820048436522484,\n",
       " 0.05420878529548645,\n",
       " 0.0015129129169508815,\n",
       " 0.027493763715028763,\n",
       " -0.0010952668963000178,\n",
       " -0.009737533517181873,\n",
       " -0.008589084260165691,\n",
       " -0.008007250726222992,\n",
       " 0.01578640006482601,\n",
       " 0.004913118667900562,\n",
       " 0.011511079035699368]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_0_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a619f9f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  13  25 100 122   7   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0  33 151 208 252 252 252 146   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0  40 152 244 252 253 224 211 252 232  40   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0  15 152 239 252 252 252 216  31  37 252 252  60   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0  96 252 252 252 252 217  29   0  37 252 252  60   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0 181 252 252 220 167  30   0   0  77 252 252  60   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0  26 128  58  22   0   0   0   0 100 252 252  60   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 157 252 252  60   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0 110 121 122 121 202 252 194   3   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0  10  53 179 253 253 255 253 253 228  35   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   5  54 227 252 243 228 170 242 252 252 231 117   6   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   6  78 252 252 125  59   0  18 208 252 252 252 252  87   7   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   5 135 252 252 180  16   0  21 203 253 247 129 173 252 252 184  66  49  49   0   0   0 \n",
      "  0   0   0   0   0   3 136 252 241 106  17   0  53 200 252 216  65   0  14  72 163 241 252 252 223   0   0   0 \n",
      "  0   0   0   0   0 105 252 242  88  18  73 170 244 252 126  29   0   0   0   0   0  89 180 180  37   0   0   0 \n",
      "  0   0   0   0   0 231 252 245 205 216 252 252 252 124   3   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0 207 252 252 252 252 178 116  36   4   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0  13  93 143 121  23   6   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guiltygyoza/opt/anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "ds = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "IDX = 5\n",
    "d = ds.data[IDX].tolist()\n",
    "d_flatten = []\n",
    "for i in range(28):\n",
    "    for j in range(28):\n",
    "        print(\"%3d\" % d[i][j], end=\" \")\n",
    "        d_flatten.append(d[i][j])\n",
    "    print()\n",
    "\n",
    "# print( len(d_flatten) )\n",
    "# print('[%s]' % ','.join( [str(e) for e in d_flatten] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "829f3983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Investigating FC1 output collected from StarkNet contract for precision loss\n",
      "282.55435953\n",
      "-164.23638096\n",
      "-4.6359771\n",
      "134.98191087\n",
      "-301.14092289\n",
      "455.0832995\n",
      "264.30724623\n",
      "-72.81002187\n",
      "-289.72881867\n",
      "-373.5267543\n",
      "-220.22185386\n",
      "-331.44123396\n",
      "-167.08183373\n",
      "-19.2201363\n",
      "257.06864873\n",
      "-36.48043006\n",
      "223.49402213\n",
      "209.49610819\n",
      "-252.46367889\n",
      "197.70606824\n",
      "-31.50400393\n",
      "864.4775756\n",
      "-323.67798672\n",
      "-141.9328909\n",
      "728.16301052\n",
      "-428.10611972\n",
      "-18.20129923\n",
      "131.29317624\n",
      "-72.73541488\n",
      "970.3215988\n",
      "-548.70506948\n",
      "495.98410122\n"
     ]
    }
   ],
   "source": [
    "print(\"> Investigating FC1 output collected from StarkNet contract for precision loss\")\n",
    "sol_s = [28255435953, -16423638096, -463597710, 13498191087, -30114092289, 45508329950, 26430724623, -7281002187, -28972881867, -37352675430, -22022185386, -33144123396, -16708183373, -1922013630, 25706864873, -3648043006, 22349402213, 20949610819, -25246367889, 19770606824, -3150400393, 86447757560, -32367798672, -14193289090, 72816301052, -42810611972, -1820129923, 13129317624, -7273541488, 97032159880, -54870506948, 49598410122]\n",
    "\n",
    "for sol in sol_s:\n",
    "    print(sol / 10**8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "36f60be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_s = range(10)\n",
    "img_array_s = []\n",
    "for idx in idx_s:\n",
    "    d = ds.data[idx].tolist()\n",
    "    d_flatten = []\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            d_flatten.append(d[i][j])\n",
    "    img_array_s.append(d_flatten)\n",
    "\n",
    "with open(\"img_array_s.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(img_array_s, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ae9586b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Investigating fixed-point precision on starknet-NN output precision\n",
      "> Scale floats by 10^16:\n",
      "-1692.6425655610296\n",
      "-2576.9741692563625\n",
      "-1252.6590447583294\n",
      "-74.59042753363163\n",
      "-2886.6927377392676\n",
      "-156.31073104193004\n",
      "-1991.9289842112619\n",
      "-2145.9652795700226\n",
      "-971.3915730007116\n",
      "-1327.1941145501023\n"
     ]
    }
   ],
   "source": [
    "sol_s = [-16926425655610295354, -25769741692563623387, -12526590447583294615, -745904275336316246, -28866927377392675394, -1563107310419300343, -19919289842112617675, -21459652795700226659, -9713915730007115903, -13271941145501024253]\n",
    "\n",
    "print(\"> Investigating fixed-point precision on starknet-NN output precision\")\n",
    "print(\"> Scale floats by 10^16:\")\n",
    "for sol in sol_s:\n",
    "    print(sol / 10**16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d260aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Scale floats by 10^8:\n",
      "-1691.22213668\n",
      "-2574.49510423\n",
      "-1251.58504463\n",
      "-74.0866657\n",
      "-2883.26465022\n",
      "-155.72522167\n",
      "-1989.93928326\n",
      "-2144.69740798\n",
      "-969.05331675\n",
      "-1325.17456612\n"
     ]
    }
   ],
   "source": [
    "sol_s = [-169122213668, -257449510423, -125158504463, -7408666570, -288326465022, -15572522167, -198993928326, -214469740798, -96905331675, -132517456612]\n",
    "\n",
    "print(\"> Scale floats by 10^8:\")\n",
    "for sol in sol_s:\n",
    "    print(sol / 10**8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6732588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
